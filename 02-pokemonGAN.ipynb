{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from skimage import io\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "from image_utils import dim_ordering_fix, dim_ordering_unfix, dim_ordering_shape\n",
    "from keras.models import Sequential\n",
    "from keras.losses import MeanAbsoluteError\n",
    "from keras.layers import Reshape, Flatten, LeakyReLU, Activation, Dense, BatchNormalization,Conv2D,Conv2DTranspose,AveragePooling2D,MaxPooling2D\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "\n",
    "\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator():\n",
    "    reg = lambda: L1L2(l1=1e-7, l2=1e-7)\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(GaussianNoise(0.1, input_shape=( 128,128,3)))\n",
    "\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    model.add(Conv2D(128, (5, 5), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    model.add(Conv2D(256, (5, 5), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    model.add(Conv2D(1, (5, 5), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(AveragePooling2D(pool_size=(4, 4), padding='valid'))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    nch = 256\n",
    "    reg = lambda: L1L2(l1=1e-7, l2=1e-7)\n",
    "    h = 5\n",
    "    n1=0.1\n",
    "    batch_norm=tf.keras.layers.experimental.SyncBatchNormalization(axis=1)\n",
    "    model.add(Dense(nch * 4 * 4, input_dim=100, kernel_regularizer=reg()))\n",
    "    model.add(batch_norm)\n",
    "    model.add(Reshape(dim_ordering_shape((nch, 4, 4))))\n",
    "    model.add(Conv2D(int(nch / 2), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(batch_norm)\n",
    "    model.add(LeakyReLU(n1))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(int(nch / 2), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(batch_norm)\n",
    "    model.add(LeakyReLU(n1))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(int(nch / 4), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(batch_norm)\n",
    "    model.add(LeakyReLU(n1))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(int(nch / 8), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(batch_norm)\n",
    "    model.add(LeakyReLU(n1))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(int(nch / 8), (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(batch_norm)\n",
    "    model.add(LeakyReLU(n1))\n",
    "\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(3, (h, h), padding='same', kernel_regularizer=reg()))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "\n",
    "    d_model.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples).reshape(n_samples,latent_dim)\n",
    "    X = g_model.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "def generate_real_fake_famples(dataset,g_model, latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples).reshape(n_samples, latent_dim)\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\n",
    "    X1 = g_model.predict(x_input)\n",
    "    y1 = np.zeros((n_samples, 1))\n",
    "\n",
    "    X2 = dataset[ix]\n",
    "    y2 = np.ones((n_samples, 1))\n",
    "\n",
    "    X=np.append(X1,X2)\n",
    "    Y=np.append(y1,y2)\n",
    "\n",
    "    X_new=np.zeros((n_samples,128,128,3))\n",
    "    Y_new=np.zeros((n_samples,1))\n",
    "    for i in range(n_samples):\n",
    "        j=np.random.randint(n_samples)\n",
    "        X_new[i]=X[j]\n",
    "        Y_new[i]=Y[j]\n",
    "\n",
    "    return X_new, Y_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_batch=16):\n",
    "    half_batch = int(n_batch / 2)\n",
    "    epochs = 10000\n",
    "    batch_size=int(dataset.shape[0] / n_batch)\n",
    "\n",
    "   # g_optim = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "  #  d_optim = SGD(lr=0.005, momentum=0.9, nesterov=True)\n",
    "\n",
    "    #g_optim = Adam(lr=0.01, beta_1=0.3)\n",
    "    #d_optim = Adam(lr=0.01, beta_1=0.3)\n",
    "\n",
    "\n",
    "\n",
    "    d_optim=RMSprop()\n",
    "    g_optim=RMSprop()\n",
    "\n",
    "    g_loss=MeanAbsoluteError()\n",
    "\n",
    "\n",
    "    gan_model.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "\n",
    "    d_model.compile(loss='binary_crossentropy', optimizer=d_optim, metrics=['categorical_accuracy'])\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for j in range(batch_size):\n",
    "\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "            X_fake, y_fake = generate_real_fake_famples(dataset,g_model, latent_dim, half_batch)\n",
    "            d_loss3, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "            X_gan = randn(latent_dim * n_batch).reshape(n_batch,latent_dim)\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            gan_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            if j %100 ==0:\n",
    "               print('epochs:%d, %d in %d, real loss=%.3f, fake loss=%.3f,fake+real loss=%.3f ,gan loss=%.3f' %\n",
    "                   (i , j , batch_size, d_loss1, d_loss2,d_loss3, gan_loss))\n",
    "               g_model.save_weights('weights/generator.hdf5', True)\n",
    "               d_model.save_weights('weights/discriminator.hdf5', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(latent_dim,n_batch):\n",
    "    generator = define_generator(latent_dim)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    generator.load_weights('generator1')\n",
    "\n",
    "    noise = randn(latent_dim * n_batch).reshape(n_batch, latent_dim)\n",
    "    # generate images\n",
    "    X = generator.predict(noise)\n",
    "\n",
    "    X = X*127.5+127.5\n",
    "    image = X.reshape((128,128,3))\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"jupit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для датасета с крупными картиночками\n",
    "def process_data(pokemon_dir,shape):\n",
    "    N = len(os.listdir(pokemon_dir))\n",
    "\n",
    "    images = np.zeros((N, shape[0],shape[1],shape[2]))\n",
    "    for i, each in enumerate(os.listdir(pokemon_dir)):\n",
    "        images[i] = io.imread(pokemon_dir + '/' + each)\n",
    "\n",
    "    images = images.astype('float32')\n",
    "\n",
    "    num_images = len(images)\n",
    "\n",
    "    return images, num_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_8 (GaussianNo (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 128, 128, 64)      4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 64, 64, 128)       204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 32, 32, 256)       819456    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 16, 16, 1)         6401      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1024)              17408     \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 1025      \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,054,082\n",
      "Trainable params: 1,054,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v1.keras.layers' has no attribute 'experimental'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-24e395334ed1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0md_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0md_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mg_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mg_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mgan_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-8f394cec8353>\u001b[0m in \u001b[0;36mdefine_generator\u001b[1;34m(latent_dim)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mn1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mbatch_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSyncBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\env_neuro\\lib\\site-packages\\tensorflow_core\\python\\util\\module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m       \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.layers' has no attribute 'experimental'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    #\n",
    "    # default generator and discriminator image shape=128,128,3\n",
    "    #\n",
    "    strategy=tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    BATCH_SIZE_PER_REPLICA=8\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "    latent_dim = 100\n",
    "    image_shape= 128, 128, 3\n",
    "    data_dir='resizedNew'\n",
    "    \n",
    "    (xtrain, ytrain) = process_data(data_dir,image_shape)\n",
    "    xtrain=xtrain.astype('float32')\n",
    "    xtrain=(xtrain-127.5)/127.5\n",
    "    \n",
    "    with strategy.scope():\n",
    "        d_model = define_discriminator()\n",
    "        d_model.summary()\n",
    "        g_model = define_generator(latent_dim)\n",
    "        g_model.summary()\n",
    "        gan_model = define_gan(g_model, d_model)\n",
    "\n",
    "    \n",
    "\n",
    "    train(g_model, d_model, gan_model, xtrain, latent_dim,BATCH_SIZE)\n",
    "\n",
    "    #generate(latent_dim,n_batch=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neuro",
   "language": "python",
   "name": "env_neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
